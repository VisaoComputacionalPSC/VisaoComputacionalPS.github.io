<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Relatório Final</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f4f4f9;
            color: #333;
        }
        h1, h2 {
            color: #0056b3;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
        }
        .section {
            margin-bottom: 30px;
        }
        ol {
            margin-left: 20px;
        }
        .botao-form {
            background-color: #4CAF50;
            color: white;
            padding: 15px 25px;
            border: none;
            border-radius: 5px;
            text-align: center;
            display: inline-block;
            text-decoration: none;
            font-size: 16px;
        }
        .botao-form:hover {
            background-color: #45a049;
        }
        
        pre {
          background: #1e1e1e;
          color: #d4d4d4;
          padding: 15px;
          border-radius: 8px;
          overflow-x: auto;
        }
    </style>
</head>
<body>

<div class="container">
    <section class="section">
        <h1>RELATÓRIO FINAL - GRUPO 2 - PSC</h1>
        
        <h2>Desenvolvedores</h2>
          <ul>

          <li >Nome: Jefferson Paiva de Paula  R.A.: 11201721192</li>
          <li> Nome: Octavio Saviano Neto      R.A.: 11201921665</li>
          <li> Nome: Tiago Cornetta Campos     R.A.: 11201922123</li>
          </ul>
      
        <h2>Introdução</h2>
        <h3>Cenário de Aplicação (CA) </h3>
         <div class="intro-text">
            <p>O <strong>Sistema de Controle de Acesso com Visão Computacional</strong> foi desenvolvido para melhorar a segurança em ambientes escolares, como salas de aula, escritórios ou outras áreas restritas. Ele utiliza tecnologia de <strong>reconhecimento facial</strong> para garantir que apenas pessoas autorizadas possam acessar locais específicos, proporcionando mais segurança para alunos, professores e colaboradores.</p>

            <p>Em muitos ambientes, os sistemas de controle de acesso ainda dependem de métodos tradicionais como cartões de identificação, senhas ou listas de presença, os quais podem ser facilmente esquecidos ou burlados. Esse sistema propõe uma solução mais eficiente e segura, que usa câmeras e inteligência artificial para reconhecer os rostos das pessoas e permitir ou bloquear a entrada automaticamente, sem intervenção humana.</p>
             
        </div>
        
        <h3>Fundamentação teórica</h3>
        <h4> Calibragem / Propriedades intrinsecas e extrinsecas</h4>
        <p>A calibração de câmeras é essencial em visão computacional, especialmente em áreas como reconstrução 3D, realidade aumentada e robótica. Ela permite corrigir distorções ópticas e associar pontos do mundo real com suas projeções em imagens.</p>        
        <p>A matriz intrínsecacontém dados como o foco da lente e o centro da imagem, transformando pontos do mundo real para coordenadas em pixels.</p>       
        <p>Os parâmetros extrínsecos, compostos pela matriz de rotação e o vetor de translação, indicam a posição e direção da câmera no espaço.</p>     
        <p>Os parâmetros de distorção corrigem imperfeições óticas da lente, como deformações nas bordas da imagem.</p>
        <p>Esses elementos permitem uma representação mais precisa dos objetos no mundo real, fundamental para várias aplicações em visão computacional.</p>

        
        
        <h4> Filtragem </h4>
        <p>A filtragem de imagens é um processo utilizado para modificar ou melhorar características específicas de uma imagem, como suavização, nitidez ou remoção de ruído.</p>
        <p>Um dos filtros mais comuns é o filtro gaussiano, que aplica uma suavização à imagem, borrando-a. Ele é baseado na função gaussiana, que dá mais peso aos pixels próximos e menos aos distantes, resultando em um efeito de desfoque suave.</p>
        <p>Esse tipo de filtragem é útil para reduzir ruídos em uma imagem e é amplamente utilizado em pré-processamento para tarefas como detecção de bordas e segmentação.</p>

        <h4> Transformação geométrica </h4>
        <p>A transformação geométrica em imagens é o processo de modificar a posição ou o tamanho de uma imagem por meio de operações matemáticas.</p>
        <p>A rotação envolve girar a imagem em torno de um ponto central, geralmente o centro da imagem. Essa operação é útil para alinhar objetos ou corrigir orientações.</p>
        <p>A translação desloca a imagem em uma direção específica, movendo-a para uma nova posição no plano. Esse tipo de transformação é frequentemente usado para ajustar o posicionamento de objetos na imagem.</p>
        <p>O redimensionamento (resize) altera o tamanho da imagem, seja para aumentá-la ou diminuí-la. Essa transformação é usada para ajustar a resolução da imagem, preservando ou modificando suas proporções.</p>


        <h4> Reconhecimento Facial </h4>     
        <p>O reconhecimento facial é a tarefa de identificar ou verificar pessoas com base em suas características faciais. É utilizado em segurança, autenticação e marketing.</p>
        <p>A biblioteca face_recognition em Python facilita o processo, permitindo detectar, reconhecer e comparar faces em imagens usando modelos de aprendizado profundo.</p>
        <p>O método Eigenfaces é uma técnica clássica que utiliza análise de componentes principais (PCA) para representar imagens faciais como combinações de características principais. O reconhecimento é feito comparando a face de entrada com essas eigenfaces.</p>
        <p>Embora o Eigenfaces seja eficiente, ele pode ser sensível a variações de iluminação e expressão. A biblioteca face_recognition usa métodos mais robustos, com melhores resultados.</p>



        

        <h2>Materiais e métodos</h2>
        <h3>Modelagem Funcional do SPV (MF)</h3>
<p>O sistema recebe o fluxo de vídeo da câmera instalada na entrada e compara os rostos detectados com o banco de dados de usuários autorizados.</p>
<ol>
  <li>Captura da imagem em tempo real.</li>
  <li>Detecção facial para localizar o rosto no quadro.</li>
  <li>Reconhecimento facial comparando com rostos cadastrados.</li>
  <li>Decisão:
    <ul>
      <li>Se o rosto for reconhecido → envia sinal para destravar a porta e registra o acesso no log.</li>
      <li>Se não for reconhecido → mantém a porta trancada, registra a tentativa e envia alerta ao administrador.</li>
    </ul>
  </li>
</ol>
<p>O administrador pode cadastrar e remover usuários, consultar o histórico e configurar alertas via interface.</p>
<p>Saídas: porta aberta ou fechada, registro de acessos e notificações em casos não auto      
        <p>O fluxo do projeto é apresentado abaixo:</p>
       <img src="Fluxo.png" alt="Fluxograma do projeto" width="600" height="400" />


        
        
        <h3>Descrição da implementação do Sistema de Processamento da Visão (SPV)</h3>
        
        <section>
          <p>
            O sistema implementado em <code>projeto.py</code> realiza o controle de acesso por reconhecimento facial, utilizando Python, OpenCV, dlib. O usuário pode calibrar a câmera, cadastrar rostos, treinar modelos e realizar o reconhecimento em tempo real.
          </p>
          <ul>
            <li>
              <strong>Filtragem de Imagens:</strong> Durante o cadastro de rostos, o sistema aplica um filtro Gaussiano para suavizar o recorte facial (<a href="#capturar_fotos_rosto"><code >cv2.GaussianBlur</code></a>), reduzindo ruídos e melhorando a qualidade das imagens para o reconhecimento.
            </li>
            <li>
              <strong>Transformações Geométricas:</strong> Os rostos detectados são redimensionados para um tamanho padrão (<a href="#capturar_fotos_rosto"><code>cv2.resize</code></a>) e as imagens passam por remapeamento para correção de distorção da lente (<a href="#capturar_fotos_rosto"><code>cv2.remap</code></a>), garantindo padronização e precisão.
            </li>
            <li>
              <strong>Calibração de Câmeras:</strong> O sistema utiliza um tabuleiro de xadrez para coletar pontos de referência, 
              calcula os parâmetros intrínsecos e extrínsecos da câmera com <a href="#calcula_params"><code>cv2.calibrateCamera</code></a> e gera mapas de desdistorção para uso posterior.
            </li>
            <li>
              <strong>Propriedades Intrínsecas e Extrínsecas:</strong> A calibração retorna matriz intrínseca, coeficientes de distorção, vetores de rotação e translação, essenciais para corrigir e entender a geometria da câmera.
            </li>
            <li>
              <strong>Reconhecimento:</strong> O sistema oferece dois métodos: <a href="#eigenfaces">Eigenfaces</a> (PCA com OpenCV) e <a href="#face_recognition">face_recognition</a> (embeddings com dlib). Ambos detectam rostos, extraem características e comparam com o banco de dados para identificar usuários autorizados.
            </li>
          </ul>
        </section>

        
        <h3>Lista dos arquivos: dos código-fonte, imagens, vídeos, e arquivos auxiliares.</h3>
        <ul>
            <li><a href="#projetopy">Código em Python</a></li>
            <li><a href="#paramsxml" >Parâmetros de calibração da câmera</a></li>
            <li><a href="#datasetfaces">Dataset de rostos cadastrados</a></li>
        </ul>
        <h3>Análise Técnica</h3>

          <ul>
            <li><strong>Captura e Calibração de Imagens:</strong> Utiliza OpenCV para detectar câmeras, capturar imagens de tabuleiro de xadrez e realizar calibração intrínseca/extrínseca, gerando mapas de correção para distorção óptica.</li>
            <li><strong>Pré-processamento:</strong> Aplica conversão para escala de cinza, redimensionamento e suavização (GaussianBlur) nas imagens faciais para melhorar a qualidade do reconhecimento.</li>
            <li><strong>Reconhecimento Facial:</strong> Implementa dois métodos:
            <ul>
            <li><strong>Eigenfaces (OpenCV):</strong> Treina um modelo com imagens do dataset e realiza reconhecimento por similaridade, exibindo confiança e nome do usuário.</li>
            <li><strong>face_recognition:</strong> Utiliza codificações faciais para comparar e identificar rostos em tempo real, mostrando nome e confiança.</li>
            </ul>
            </li>
            <li><strong>Organização do Dataset:</strong> Gerencia diretórios e arquivos para armazenar imagens capturadas e resultados do processamento.</li>
            <li><strong>Validação e Feedback:</strong> Exibe mensagens informativas, status de operações e resultados diretamente na interface e no terminal.</li>
          </ul>
        <h3>Código fonte</h3>
        <p>O código fonte pode ser encontrado no arquivo <a href="#projetopy">projeto.py</a></p>
        <h2>detectar_cameras(max_cameras=10)</h2>
        <p>Varre índices de 0 até <code>max_cameras-1</code>, abre cada dispositivo com OpenCV e adiciona à lista se um frame válido é lido. Retorna a lista de índices detectados.</p>
        <pre><code class="language-python">def detectar_cameras(max_cameras=10):
            print("Procurando por câmeras conectadas...")
            indices_disponiveis = []
            for i in range(max_cameras):
                cap = cv2.VideoCapture(i)
                if cap.read()[0]:
                    print(f"Câmera encontrada no índice {i}")
                    indices_disponiveis.append(i)
                cap.release()
            return indices_disponiveis
        </code></pre>

        <h2>tirar_fotos()</h2>
        <p>Usa a primeira câmera encontrada, mostra um contador de 2s, tenta detectar um tabuleiro de xadrez 8×6 e, quando detectado no momento do disparo, salva até 20 imagens para calibração.</p>
        <pre><code class="language-python">def tirar_fotos():

            indices = detectar_cameras()
            if len(indices) &lt; 1:
                print("Menos de uma câmera detectadas. Conecte duas e tente novamente.")
                return

            CamL_id = indices[0]

            CamL = cv2.VideoCapture(CamL_id)

            for i in range(100):
                retL, frameL = CamL.read()

            cv2.imshow('Grupo PSC', frameL)
            CamL.release()

            CamL = cv2.VideoCapture(CamL_id)

            start = time.time()
            T = 2
            count = 0

            while True:
                timer = T - int(time.time() - start)
                retL, frameL = CamL.read()

                img1_temp = frameL.copy()
                cv2.putText(img1_temp, f"{timer}", (50, 50), 1, 5, (55, 0, 0), 5)
                cv2.imshow('Grupo PSC', img1_temp)

                grayL = cv2.cvtColor(frameL, cv2.COLOR_BGR2GRAY)

                retL, cornersL = cv2.findChessboardCorners(grayL, (8, 6), None)

                if (retL == True) and timer &lt;= 0 and count &lt;20:
                    count += 1
                    cv2.imwrite(f'{path}/img{count}.png', frameL)

                if timer &lt;= 0:
                    start = time.time()

                if (cv2.waitKey(1) &amp; 0xFF == 27) or (count&gt;=20) :
                    print("Closing the cameras!")
                    break

            CamL.release()
            cv2.destroyAllWindows()
        </code></pre>

        <h2>calibragem()</h2>
        <p>Para cada imagem salva (img1..img20), detecta cantos do tabuleiro 8×6, refina com <code>cornerSubPix</code>, desenha e acumula pares 3D–2D. Ao final chama <code>calcula_params</code>.</p>
        <pre><code class="language-python">def calibragem():
            print("Extracting image coordinates of respective 3D pattern ....\n")

            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)

            objp = np.zeros((8 * 6, 3), np.float32)
            objp[:, :2] = np.mgrid[0:8, 0:6].T.reshape(-1, 2)

            img_ptsL = []
            img_ptsR = []
            obj_pts = []

            for i in tqdm(range(1, 21)):
                imgL = cv2.imread(f"{path}/img{i}.png")

                imgL_gray = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)

                outputL = imgL.copy()

                retL, cornersL = cv2.findChessboardCorners(outputL, (8, 6), None)

                if retL:
                    obj_pts.append(objp)
                    cv2.cornerSubPix(imgL_gray, cornersL, (11, 11), (-1, -1), criteria)
                    cv2.drawChessboardCorners(outputL, (8, 6), cornersL, retL)
                    #cv2.imshow('Grupo PSC', outputL)

                    img_ptsL.append(cornersL)

                    cv2.imwrite(f'{path}/img{i}.png', outputL)

            calcula_params(obj_pts, img_ptsL, imgL_gray)
        </code></pre>

        <h2 id="calcula_params">calcula_params(obj_pts, img_ptsL, imgL_gray)</h2>
        <p>Roda <code>cv2.calibrateCamera</code> para obter intrínsecos e distorção. Cria mapas de remapeamento com <code>initUndistortRectifyMap</code> e salva em <code>params_py.xml</code>.</p>
        <pre><code class="language-python">def calcula_params(obj_pts, img_ptsL, imgL_gray):
            print("Calculating left camera parameters ... ")
            retL, mtxL, distL, rvecsL, tvecsL = <strong>cv2.calibrateCamera</strong>(obj_pts, img_ptsL, imgL_gray.shape[::-1], None, None)

            Left_Stereo_Map = cv2.initUndistortRectifyMap(
                mtxL, distL, None, mtxL, imgL_gray.shape[::-1], cv2.CV_16SC2
            )

            print("Saving parameters ......")
            cv_file = cv2.FileStorage(f"{path}/params_py.xml", cv2.FILE_STORAGE_WRITE)
            cv_file.write("Left_Stereo_Map_x", Left_Stereo_Map[0])
            cv_file.write("Left_Stereo_Map_y", Left_Stereo_Map[1])
            cv_file.release()
        </code></pre>

        <h2>criar_diretorio_dataset()</h2>
        <p>Garante que a pasta <code>dataset_faces</code> exista para salvar os recortes de rosto.</p>
        <pre><code class="language-python">def criar_diretorio_dataset():
            if not os.path.exists(dataset_path):
                os.makedirs(dataset_path)
        </code></pre>

        <h2 id="capturar_fotos_rosto">capturar_fotos_rosto()</h2>
        <p>Lê o nome do usuário, carrega os mapas de remapeamento da calibração, detecta um rosto (Haar Cascade), faz um countdown de 3s, recorta, suaviza e salva uma imagem 200×200 em <code>dataset_faces/&lt;usuario&gt;_0.png</code>.</p>
        <pre><code class="language-python">def capturar_fotos_rosto():
            num_fotos = 1
            nome_usuario = input("Digite o nome do usuário para capturar as fotos: ").strip()
            if not nome_usuario:
                print("[ERRO] Nome de usuário inválido!")
                return

            cv_file = cv2.FileStorage(f"{path}/params_py.xml", cv2.FILE_STORAGE_READ)
            Left_Stereo_Map_x = cv_file.getNode("Left_Stereo_Map_x").mat()
            Left_Stereo_Map_y = cv_file.getNode("Left_Stereo_Map_y").mat()
            cv_file.release()

            indices = detectar_cameras()
            if len(indices) &lt; 1:
                print("❌ Nenhuma câmera detectada.")
                return

            CamL = cv2.VideoCapture(indices[0])
            criar_diretorio_dataset()

            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
            print(f"[INFO] Capturando {num_fotos} fotos de {nome_usuario}...")

            count = 0
            countdown_time = 3
            start_time = None

            while count &lt; num_fotos:
                retL, frameL = CamL.read()
                if not retL:
                    continue

                frameL = cv2.remap(frameL, Left_Stereo_Map_x, Left_Stereo_Map_y, cv2.INTER_LINEAR)
                #grayL = cv2.cvtColor(frameL, cv2.COLOR_BGR2GRAY)
                grayL = frameL
                facesL = face_cascade.detectMultiScale(grayL, 1.3, 5)

                rosto = None
                if len(facesL) &gt; 0:
                    if start_time is None:
                        start_time = time.time()
                    elapsed = time.time() - start_time
                    countdown = countdown_time - int(elapsed)

                    (x, y, w, h) = facesL[0]
                    rosto = grayL[y:y+h, x:x+w]
                    rosto = cv2.resize(rosto, (200, 200))
                    rosto = cv2.GaussianBlur(rosto, (5, 5), 0)  # Suavização de ruído
                    cv2.imshow("PSC - Rosto Suavizado", rosto)  # Exibe o rosto suavizado
                    cv2.rectangle(frameL, (x, y), (x+w, y+h), (0, 255, 0), 2)
                    cv2.putText(frameL, f"Contagem: {countdown}", (10, 30),
                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                else:
                    start_time = None
                    countdown = countdown_time

                cv2.putText(frameL, f"Foto {count+1}/{num_fotos}", (10, frameL.shape[0]-10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

                cv2.imshow("Grupo PSC", frameL)

                if rosto is not None and start_time is not None and elapsed &gt;= countdown_time:
                    # Salvar com nome do usuário, e incrementar índice para não substituir
                    foto_path = f"{dataset_path}/{nome_usuario}_{count}.png"
                    cv2.imwrite(foto_path, rosto)
                    print(f"[INFO] Foto {count+1} salva em {foto_path}")
                    count += 1
                    start_time = None
                    time.sleep(0.5)

                if cv2.waitKey(1) &amp; 0xFF == 27:
                    print("[INFO] Captura interrompida.")
                    break

            CamL.release()
            cv2.destroyAllWindows()
            print(f"[INFO] Captura de fotos para {nome_usuario} concluída.")
        </code></pre>

        <h2 id="eigenfaces">treinar_eigenfaces()</h2>
        <p>Lê imagens do dataset, converte para tons de cinza, cria rótulos a partir do prefixo do arquivo, treina um <code>EigenFaceRecognizer</code> e retorna o reconhecedor e o mapa de rótulos.</p>
        <pre><code class="language-python">def treinar_eigenfaces():
            """
            Carrega as imagens do dataset, treina o modelo Eigenfaces e retorna o reconhecedor treinado.
            """
            print("[INFO] Carregando dataset e treinando Eigenfaces...")

            faces = []
            labels = []
            label_map = {}
            current_label = 0

            path_str = f"{dataset_path}/"

            for filename in os.listdir(path_str):
                if filename.endswith(".png") or filename.endswith(".jpg"):
                    img_path = os.path.join(path_str, filename)
                    # Lê a imagem e converte para grayscale se necessário
                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
                    if img is None:
                        print(f"[WARN] Não foi possível carregar a imagem {img_path}")
                        continue

                    # Extrai o nome do usuário do nome do arquivo
                    nome_usuario = filename.rsplit('_', 1)[0]

                    if nome_usuario not in label_map:
                        label_map[nome_usuario] = current_label
                        current_label += 1

                    faces.append(img)
                    labels.append(label_map[nome_usuario])

            if len(faces) == 0:
                print("[ERRO] Dataset vazio! Capture fotos antes de treinar.")
                return None, None

            recognizer = cv2.face.EigenFaceRecognizer_create()
            recognizer.train(faces, np.array(labels))

            print("[INFO] Treinamento concluído.")
            return recognizer, label_map
        </code></pre>

        <h2>reconhecer_rosto_eigenface(recognizer, label_map)</h2>
        <p>Abre a webcam, aplica remapeamento da calibração, detecta rostos, redimensiona para 200×200, faz <code>predict</code> no reconhecedor Eigenfaces e escreve o nome/“desconhecido” no frame.</p>
        <pre><code class="language-python">def reconhecer_rosto_eigenface(recognizer, label_map):
            """
            Captura imagem da webcam, aplica calibração com remap, detecta o rosto e tenta reconhecê-lo usando o modelo Eigenfaces.
            """

            if recognizer is None or label_map is None:
                print("[ERRO] Modelo não treinado!")
                return

            # === Ler parâmetros de calibração (remap) ===
            print("[DEBUG] Carregando parâmetros de calibração...")
            cv_file = cv2.FileStorage(f"{path}/params_py.xml", cv2.FILE_STORAGE_READ)
            Left_Stereo_Map_x = cv_file.getNode("Left_Stereo_Map_x").mat()
            Left_Stereo_Map_y = cv_file.getNode("Left_Stereo_Map_y").mat()
            cv_file.release()
            print("[DEBUG] Mapas de calibração carregados com sucesso.")

            # === Inicializar câmera ===
            cam = cv2.VideoCapture(0)
            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

            print("[INFO] Posicione seu rosto na frente da câmera para reconhecimento.")

            while True:
                ret, frame = cam.read()
                if not ret:
                    print("[ERRO] Falha ao capturar frame da câmera.")
                    continue

                # === Aplicar calibração com remap ===
                frame_corrigido = cv2.remap(frame, Left_Stereo_Map_x, Left_Stereo_Map_y, cv2.INTER_LINEAR)

                gray = cv2.cvtColor(frame_corrigido, cv2.COLOR_BGR2GRAY)
                faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)

                print(f"[DEBUG] {len(faces)} rosto(s) detectado(s).")

                for i, (x, y, w, h) in enumerate(faces):
                    rosto = gray[y:y+h, x:x+w]
                    rosto = cv2.resize(rosto, (200, 200))

                    # Exibe o rosto isolado
                    cv2.imshow(f"Grupo PSC {i+1}", rosto)

                    # Salva imagem do rosto para inspeção
                    debug_img_path = f"debug_rosto_predito_{i+1}.png"
                    cv2.imwrite(debug_img_path, rosto)
                    print(f"[DEBUG] Rosto {i+1} salvo como {debug_img_path}")

                    # === Predição ===
                    label_id, confianca = recognizer.predict(rosto)
                    print(f"[DEBUG] Rosto {i+1} - Label predito: {label_id}, Confiança: {confianca:.2f}")

                    # === Verifica confiança ===
                    limiar_confianca = 3500  # Ajuste idealmente com validação
                    if confianca &gt; limiar_confianca:
                        texto = f"Desconhecido - Confiança: {confianca:.2f}"
                        print("[DEBUG] Reconhecimento falhou (desconhecido)")
                    else:
                        nome_usuario = None
                        for nome, idx in label_map.items():
                            if idx == label_id:
                                nome_usuario = nome
                                break
                        texto = f"{nome_usuario} - Confiança: {confianca:.2f}"
                        print(f"[DEBUG] Reconhecido como {nome_usuario}")

                    # === Mostrar na tela ===
                    cv2.rectangle(frame_corrigido, (x, y), (x + w, y + h), (0, 255, 0), 2)
                    cv2.putText(frame_corrigido, texto, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

                # === Mostrar imagem final com anotações ===
                cv2.imshow("Grupo PSC - Reconhecimento Facial (Eigenfaces)", frame_corrigido)

                if cv2.waitKey(1) &amp; 0xFF == 27:  # ESC para sair
                    print("[INFO] Reconhecimento encerrado pelo usuário.")
                    break

            cam.release()
            cv2.destroyAllWindows()
        </code></pre>

        <h2 id="face_recognition">reconhecer_rosto_face_recognition()</h2>
        <p>Carrega encodes do dataset com a lib <code>face_recognition</code>, abre webcam, localiza rostos, compara distâncias para identificar o mais próximo e escreve nome e “confiança” no frame.</p>
        <pre><code class="language-python">def reconhecer_rosto_face_recognition():
            dataset_path = "dataset_faces"

            print("[INFO] Carregando imagens do dataset para reconhecimento...")
            imagens = []
            nomes = []
            codificacoes = []

            # Carregar imagens e nomes
            for arquivo in os.listdir(dataset_path):
                if arquivo.endswith(".png") or arquivo.endswith(".jpg"):
                    caminho_imagem = os.path.join(dataset_path, arquivo)
                    img = face_recognition.load_image_file(caminho_imagem)
                    codificacao = face_recognition.face_encodings(img)
                    if len(codificacao) &gt; 0:
                        codificacoes.append(codificacao[0])
                        nome_usuario = arquivo.rsplit('_', 1)[0]
                        nomes.append(nome_usuario)
                    else:
                        print(f"[WARN] Nenhum rosto encontrado na imagem {arquivo}")

            if len(codificacoes) == 0:
                print("[ERRO] Nenhuma codificação facial válida encontrada no dataset!")
                return

            indices = detectar_cameras()
            if len(indices) &lt; 1:
                print("❌ Nenhuma câmera detectada.")
                return

            CamL = cv2.VideoCapture(indices[0])

            print("[INFO] Iniciando reconhecimento. Pressione ESC para sair.")

            while True:
                ret, frame = CamL.read()
                if not ret:
                    continue

                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                faces = face_recognition.face_locations(rgb_frame)
                codificacoes_frame = face_recognition.face_encodings(rgb_frame, faces)

                for (top, right, bottom, left), codificacao_rosto in zip(faces, codificacoes_frame):
                    resultados = face_recognition.compare_faces(codificacoes, codificacao_rosto)
                    distancias = face_recognition.face_distance(codificacoes, codificacao_rosto)
                    melhor_indice = np.argmin(distancias) if len(distancias) &gt; 0 else None

                    if melhor_indice is not None and resultados[melhor_indice]:
                        nome = nomes[melhor_indice]
                        confianca = 1 - distancias[melhor_indice]  # Confiança inversa da distância
                    else:
                        nome = "Desconhecido"
                        confianca = 0

                    cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
                    cv2.putText(frame, f"{nome} ({confianca:.2f})", (left, top - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

                cv2.imshow("Grupo PSC - Reconhecimento Face_Recognition", frame)

                if cv2.waitKey(1) &amp; 0xFF == 27:  # ESC para sair
                    break

            CamL.release()
            cv2.destroyAllWindows()
        </code></pre>

        <h2>run()</h2>
        <p>Fluxo em modo terminal: calibração (com captura, se necessário), captura de rosto (se necessário), treinamento Eigenfaces e escolha do método de reconhecimento (Eigenfaces ou face_recognition).</p>
        <pre><code class="language-python">def run():
            cal = input("Já tirou foto? (s/n) ")
            if cal == "n":
                tirar_fotos()
                calibragem()
            else:
                calibragem()

            face = input("Já tirou foto face? (s/n) ")
            if face == "n":
                capturar_fotos_rosto()
                recognizer, label_map = treinar_eigenfaces()
            else:
                recognizer, label_map = treinar_eigenfaces()

            metodo = input("Escolha o método de reconhecimento:\n1 - Eigenfaces\n2 - face_recognition\nDigite 1 ou 2: ")

            if metodo == "1":
                reconhecer_rosto_eigenface(recognizer, label_map)
            elif metodo == "2":
                reconhecer_rosto_face_recognition()
            else:
                print("Método inválido. Encerrando.")
        </code></pre>

        <section>
            <h2>O que é Eigenfaces (OpenCV)</h2>
            <p><strong>Ideia central:</strong> aplicar PCA (Análise de Componentes Principais) em imagens de rosto para encontrar uma base de “eigenfaces” que explicam a maior parte da variância do conjunto. Novas imagens são projetadas nesse subespaço e classificadas por distância ao vizinho mais próximo.</p>
            <h3>Como funciona (passo a passo)</h3>
            <ol>
              <li><strong>Vetorização:</strong> transformar a imagem 2D do rosto em um vetor 1D.</li>
              <li><strong>Face média:</strong> subtrair a média para minimizar viés de iluminação global.</li>
              <li><strong>PCA:</strong> obter autovetores (eigenfaces) da matriz de covariância; eles formam a base do subespaço.</li>
              <li><strong>Projeção:</strong> projetar a imagem no subespaço para obter um vetor de coeficientes.</li>
              <li><strong>Classificação:</strong> comparar por distância (tipicamente Euclidiana) aos vetores projetados do treino.</li>
              <li><strong>“Confiança” (OpenCV):</strong> valor de <em>distância</em> (quanto menor, melhor). Não é probabilidade.</li>
            </ol>
            <h3>Uso no código</h3>
            <ul>
              <li><strong>Treino:</strong> carrega imagens do <code>dataset_faces/</code>, converte para escala de cinza e treina com <code>cv2.face.EigenFaceRecognizer_create()</code>.</li>
              <li><strong>Predição:</strong> detecta o rosto, recorta, redimensiona para <code>200×200</code> e chama <code>recognizer.predict()</code>. Um limiar (ex.: <code>3500</code>) decide “desconhecido”.</li>
            </ul>
            <h3>Prós</h3>
            <ul>
              <li>Simples, rápido e explicável.</li>
              <li>Requer pouco dado por pessoa (em ambientes controlados).</li>
            </ul>
            <h3>Contras</h3>
            <ul>
              <li>Sensível a iluminação, pose e oclusões.</li>
              <li>Pressupõe alinhamento e padronização fortes.</li>
              <li>Limiar de “confiança” precisa ser ajustado empiricamente.</li>
            </ul>
            <h3>Boas práticas</h3>
            <ul>
              <li>Padronizar pré-processamento (grayscale, equalização de histograma, alinhamento por olhos).</li>
              <li>Garantir mesmo tamanho de entrada no treino e na predição.</li>
              <li>Ajustar o limiar medindo FAR/FRR em validação.</li>
            </ul>
          </section>
          
          <hr />
          
          <section>
            <h2>O que é <code>face_recognition</code> (dlib)</h2>
            <p><strong>Ideia central:</strong> transformar cada rosto em um <strong>vetor de 128 dimensões</strong> usando uma rede neural (ResNet) treinada com perda métrica. Rostos da mesma pessoa ficam próximos nesse espaço; diferentes, distantes.</p>
            <h3>Como funciona (passo a passo)</h3>
            <ol>
              <li><strong>Detecção:</strong> localizar rostos (HOG por padrão; pode usar CNN).</li>
              <li><strong>Landmarks e alinhamento:</strong> estimar pontos faciais e alinhar o rosto.</li>
              <li><strong>Embedding 128-D:</strong> extrair o vetor descritivo (assinatura) do rosto.</li>
              <li><strong>Comparação:</strong> calcular distância Euclidiana entre embeddings; <code>compare_faces</code> usa tolerância típica ≈ <code>0.6</code>.</li>
            </ol>
            <h3>Uso no código</h3>
            <ul>
              <li><strong>Pré-processamento:</strong> carrega todas as imagens do dataset, extrai um embedding por imagem e armazena junto com o nome.</li>
              <li><strong>Execução:</strong> para cada rosto no frame atual, calcula o embedding e compara com o banco (menor distância vence). Exibe nome se a distância passar no critério de tolerância.</li>
              <li><strong>Observação:</strong> a “confiança” exibida como <code>1 - distância</code> é apenas heurística visual; não é probabilidade calibrada.</li>
            </ul>
            <h3>Prós</h3>
            <ul>
              <li>Bem mais robusto a variações de iluminação/pose moderadas.</li>
              <li>Expansão simples: basta adicionar novas imagens (novos embeddings).</li>
              <li>Não exige treinar um classificador por pessoa.</li>
            </ul>
            <h3>Contras</h3>
            <ul>
              <li>Embedding é mais custoso computacionalmente que PCA.</li>
              <li>A tolerância ideal (ex.: 0.6) precisa ser ajustada ao seu ambiente.</li>
            </ul>
            <h3>Boas práticas</h3>
            <ul>
              <li>Usar múltiplas fotos por pessoa (ângulos/expressões/iluminações diferentes).</li>
              <li>Ajustar a tolerância com validação local (ex.: 0.45–0.70).</li>
              <li>Se precisar de mais precisão na detecção, considerar o detector CNN (mais pesado).</li>
            </ul>
          </section>
          
          <hr />
          
          <section>
            <h2>Quadro de Comparação</h2>
            <table border="1" cellpadding="6" cellspacing="0">
              <thead>
                <tr>
                  <th>Aspecto</th>
                  <th>Eigenfaces (OpenCV)</th>
                  <th>face_recognition (dlib)</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Representação</td>
                  <td>PCA (subespaço linear de “eigenfaces”)</td>
                  <td>Embedding 128-D por rede neural (ResNet)</td>
                </tr>
                <tr>
                  <td>Treinamento</td>
                  <td>Treina modelo com todo o dataset (calcula base PCA)</td>
                  <td>Não treina por pessoa; apenas extrai embeddings e armazena</td>
                </tr>
                <tr>
                  <td>Métrica / Saída</td>
                  <td>Distância no subespaço; “confidence” é distância (menor=melhor)</td>
                  <td>Distância Euclidiana entre embeddings; <code>compare_faces</code> usa tolerância (≈0.6)</td>
                </tr>
                <tr>
                  <td>Robustez</td>
                  <td>Baixa para variações de luz/pose/oclusão</td>
                  <td>Alta para variações moderadas</td>
                </tr>
                <tr>
                  <td>Pré-processamento</td>
                  <td>Exige forte padronização (grayscale, alinhamento, tamanho fixo)</td>
                  <td>Alinhamento via landmarks embutido; ainda ajuda padronizar</td>
                </tr>
                <tr>
                  <td>Dados por pessoa</td>
                  <td>Poucos já funcionam, mas ambiente precisa ser controlado</td>
                  <td>Quanto mais diversidade, melhor generalização</td>
                </tr>
                <tr>
                  <td>Velocidade</td>
                  <td>Muito rápido (projeção em PCA)</td>
                  <td>Razoável (custo do embedding é maior)</td>
                </tr>
                <tr>
                  <td>Ajuste de decisão</td>
                  <td>Limiar empírico da distância (ex.: 3500 no seu código)</td>
                  <td>Tolerância empírica (ex.: 0.6); ajustar conforme FAR/FRR</td>
                </tr>
                <tr>
                  <td>Escalabilidade</td>
                  <td>Adicionar pessoas pode exigir re-treino do PCA</td>
                  <td>Basta adicionar novos embeddings; não requer re-treino global</td>
                </tr>
              </tbody>
            </table>
          </section>
          
          <hr />

        <section>
        <h2>Demonstração de Uso</h2>
        <p>Para visualizar a demonstração prática da aplicação dos métodos de reconhecimento facial, acesse o link abaixo:</p>
        <p><a href="https://photos.app.goo.gl/UNDrF7m7sBGz65N3A" target="_blank">▶ Ver Demonstração</a></p>
        </section>
      
        <h2>Laboratorio Experimental</h2>
        <p> O teste em campo ocorreu no dia 11/08/2025 e disponibilizamos o seguinte passo a passo para as pessoas abaixo. </p>
        <h3>Roteiro do Laboratório Experimental</h3>
        <section class="section">
        <p>Siga os passos abaixo para operar o sistema e coletar os resultados:</p>
        <ol>
            <li><strong>Abrir o sistema:</strong> Ao abrir o sistema apresenta a opção de tirar as fotos para calibrar a 
                câmera, se selecionar sim:</li>
            <ol type="a">
                <li>Ficar na frente da camera com o tabuleiro para tirar as fotos</li>
                <li>A <mark><strong>Calibração</strong></mark> é feita automaticamente</li>
            </ol>
            <li><strong>Cadastro de usuário: </strong> Se for necessário cadastrar um novo usuário, selecionar sim:</li>
                <ol type="a">
                    <li>Aparecer o rosto na frente da câmera até a mesma tirar a foto</li>
                    <li>Para salvar é solicitado o nome do usuário para cadastro</li>
                    <li>Nesse momento é feita a <mark><strong>Filtragem</strong></mark> e <mark><strong>Tranformação Geométrica</strong></mark>   </li>
                </ol>
            <li><strong>Escolher Método de reconhecimento facial:</strong> Após configurar os parâmetros, seleciona qual o método vai ser usado.</li>
                <ol type="a">
                    <li>Opção 1 = Eingfaces</li>
                    <li>Opção 2 = Face_Recognition</li>
                    <li>Após a escolha será iniciada o reconhecimento</li>
                </ol>
            <li><strong>Reconhecimento:</strong> Visualize a performance do sistema na tela, onde mostra o grau de certeza que foi localizado.</li>
                <ol type="a">
                    <li>Do contrário, caso não reconhecido, deve aparecer como "desconhecido"</li>
                    <li>Na prática o acesso seria negado para esse usuário</li>
                </ol>
        </ol>

        <p>Por favor, clique no link abaixo para acessar o questionário de avaliação e responder às questões sobre o experimento:</p>
        <a href="https://docs.google.com/forms/d/1RQewnyRig8O_KLCCyn0REd_ARzE5OrtFDU3Ue18dLPw/edit" target="_blank" class="botao-form">Acessar Questionário</a>
    </section>

        <h1>Quadro de Correspondência: Requisitos ↔ Trechos do Código</h1>

<h2>I. Requisitos Obrigatórios</h2>
<table border="1" cellpadding="6" cellspacing="0">
  <thead>
    <tr>
      <th>Requisito</th>
      <th>Onde no código</th>
      <th>Trecho correspondente</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Filtragem de imagens</td>
      <td><code>capturar_fotos_rosto()</code> — suavização do recorte do rosto</td>
      <td>
        <pre><code class="language-python">rosto = cv2.resize(rosto, (200, 200))
rosto = cv2.GaussianBlur(rosto, (5, 5), 0)  # Suavização de ruído</code></pre>
      </td>
    </tr>

    <tr>
      <td>Transformações geométricas</td>
      <td>
        <p>a) Redimensionamento do rosto (padronização)</p>
        <p>b) Remapeamento (desdistorção por mapa)</p>
      </td>
      <td>
        <pre><code class="language-python"># a) Em capturar_fotos_rosto() e reconhecer_rosto_eigenface()
rosto = cv2.resize(rosto, (200, 200))</code></pre>
        <pre><code class="language-python"># b) Em reconhecer_rosto_eigenface() (aplica mapas gerados na calibração)
frame_corrigido = cv2.remap(frame, Left_Stereo_Map_x, Left_Stereo_Map_y, cv2.INTER_LINEAR)</code></pre>
      </td>
    </tr>

    <tr>
      <td>Calibração de câmeras</td>
      <td>
        <p>a) Coleta de cantos do tabuleiro (calibração)</p>
        <p>b) Estimativa de parâmetros e mapas</p>
      </td>
      <td>
        <pre><code class="language-python"># a) Em calibragem()
retL, cornersL = cv2.findChessboardCorners(outputL, (8, 6), None)
cv2.cornerSubPix(imgL_gray, cornersL, (11, 11), (-1, -1), criteria)
img_ptsL.append(cornersL)
obj_pts.append(objp)</code></pre>
        <pre><code class="language-python"># b) Em calcula_params()
retL, mtxL, distL, rvecsL, tvecsL = cv2.calibrateCamera(
    obj_pts, img_ptsL, imgL_gray.shape[::-1], None, None)

Left_Stereo_Map = cv2.initUndistortRectifyMap(
    mtxL, distL, None, mtxL, imgL_gray.shape[::-1], cv2.CV_16SC2)

cv_file = cv2.FileStorage(f"{path}/params_py.xml", cv2.FILE_STORAGE_WRITE)
cv_file.write("Left_Stereo_Map_x", Left_Stereo_Map[0])
cv_file.write("Left_Stereo_Map_y", Left_Stereo_Map[1])
cv_file.release()</code></pre>
      </td>
    </tr>

    <tr>
      <td>Propriedades intrínsecas e extrínsecas</td>
      <td><code>calcula_params()</code> — retornos de <code>calibrateCamera</code></td>
      <td>
        <pre><code class="language-python">retL, mtxL, distL, rvecsL, tvecsL = cv2.calibrateCamera(
    obj_pts, img_ptsL, imgL_gray.shape[::-1], None, None)
# mtxL  -> matriz intrínseca
# distL -> coeficientes de distorção (intrínsecos)
# rvecsL, tvecsL -> extrínsecos (rotação e translação por imagem)</code></pre>
      </td>
    </tr>
  </tbody>
</table>

<h2>II. Pelo menos um item (escolhido: Reconhecimento)</h2>
<table border="1" cellpadding="6" cellspacing="0">
  <thead>
    <tr>
      <th>Item</th>
      <th>Onde no código</th>
      <th>Trecho correspondente</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Reconhecimento — Eigenfaces (OpenCV)</td>
      <td>
        <p>a) Treinamento do reconhecedor</p>
        <p>b) Predição no frame</p>
      </td>
      <td>
        <pre><code class="language-python"># a) Em treinar_eigenfaces()
recognizer = cv2.face.EigenFaceRecognizer_create()
recognizer.train(faces, np.array(labels))</code></pre>
        <pre><code class="language-python"># b) Em reconhecer_rosto_eigenface()
label_id, confianca = recognizer.predict(rosto)
limiar_confianca = 3500
if confianca &gt; limiar_confianca:
    texto = f"Desconhecido - Confiança: {confianca:.2f}"
else:
    # mapeia label_id para nome</code></pre>
      </td>
    </tr>

    <tr>
      <td>Reconhecimento — face_recognition (dlib)</td>
      <td>
        <p>a) Extração de embeddings do dataset</p>
        <p>b) Comparação de embeddings na webcam</p>
      </td>
      <td>
        <pre><code class="language-python"># a) Em reconhecer_rosto_face_recognition() (pré-carregamento)
img = face_recognition.load_image_file(caminho_imagem)
codificacao = face_recognition.face_encodings(img)
if len(codificacao) &gt; 0:
    codificacoes.append(codificacao[0])</code></pre>
        <pre><code class="language-python"># b) Em reconhecer_rosto_face_recognition() (loop da câmera)
faces = face_recognition.face_locations(rgb_frame)
codificacoes_frame = face_recognition.face_encodings(rgb_frame, faces)
resultados = face_recognition.compare_faces(codificacoes, codificacao_rosto)
distancias = face_recognition.face_distance(codificacoes, codificacao_rosto)
melhor_indice = np.argmin(distancias) if len(distancias) &gt; 0 else None</code></pre>
      </td>
    </tr>
  </tbody>
</table>

</div>

    <h3>Testes Realizados</h3>
    <p>
    Nos testes, primeiro abrimos o sistema e fizemos a calibração da câmera usando o tabuleiro. 
    O processo foi automático e deixou a câmera pronta para uso.
    </p>
    <p>
    Depois, realizamos o cadastro de usuários. 
    A pessoa ficava em frente à câmera, que tirava a foto automaticamente. 
    Em seguida, o sistema pedia o nome para salvar o registro.
    </p>
    <p>
    Na sequência, escolhemos o método de reconhecimento: 
    Eigenfaces ou Face_Recognition._
    </p>
    <p>
    Ao final, foi disponibilizado um questionário de avaliação 
    para que os participantes dessem sua opinião sobre o experimento.
    </p>


    <h3>Questionário de Avaliação</h3>
    <p>
    Foi elaborado um questionário para avaliar a percepção dos participantes sobre o sistema de controle de acesso com visão computacional.
    O objetivo foi verificar se os conceitos principais foram compreendidos e coletar opiniões sobre o funcionamento do sistema.
    </p>
    
    <p>As perguntas aplicadas foram:</p>
    <ol>
        <li>O que é um sistema de controle de acesso baseado em visão computacional?</li>
        <li>Qual é a principal vantagem de utilizar visão computacional em sistemas de controle de acesso?</li>
        <li>Qual tecnologia de visão computacional é geralmente utilizada para identificar as pessoas em um sistema de controle de acesso?</li>
        <li>Qual fator pode prejudicar a precisão do reconhecimento facial em um sistema de controle de acesso?</li>
        <li>Em um ambiente com pouca luz, o sistema de controle de acesso com visão computacional pode enfrentar dificuldades devido a:</li>
        <li>Caso o sistema de controle de acesso não consiga identificar uma pessoa, o que normalmente acontece?</li>
    </ol>
    
    <p>
    Com essas perguntas, buscamos avaliar o entendimento dos participantes e coletar feedback sobre a clareza e eficiência do experimento.
    </p>
    
        <h3>Análise dos Resultados do Teste de Campo TCS</h3>
        
        <p>
        Com base no questionário aplicado, foram consideradas apenas as respostas dos participantes que obtiveram nota registrada. 
        Abaixo estão os resultados individuais:
        </p>
        
        <div style="display: flex; justify-content: center; align-items: center; width: 100%; margin: 24px 0;">
        <table border="1" cellpadding="8" cellspacing="0">
            <tr>
                <th>R.A.</th>
                <th>Nome</th>
                <th>Nota</th>
            </tr>
            <tr>
                <td>11201921101</td>
                <td>CAIO VILOR BRANDAO</td>
                <td>10,0</td>
            </tr>
            <tr>
                <td>11201921175</td>
                <td>GUILHERME DE SOUSA SANTOS</td>
                <td>8,3</td>
            </tr>
            <tr>
                <td>11202130906</td>
                <td>GUILHERME DO AMARAL</td>
                <td>10,0</td>
            </tr>
            <tr>
                <td>11202020351</td>
                <td>IAN VICTOR TONIOLO SILVA</td>
                <td>10,0</td>
            </tr>
            <tr>
                <td>11202320802</td>
                <td>IGOR DOMINGOS DA SILVA MOZETIC</td>
                <td>10,0</td>
            </tr>
            <tr>
                <td>11202320245</td>
                <td>JHONATAN FERREIRA MACHADO</td>
                <td>8,3</td>
            </tr>
            <tr>
                <td>11058715</td>
                <td>JORGE LUIZ PINTO JUNIOR</td>
                <td>10,0</td>
            </tr>
            <tr>
                <td>11201920579</td>
                <td>LEONARDO SEVERGNINE MAIOLI</td>
                <td>10,0</td>
            </tr>
            <tr>
                <td>11201922156</td>
                <td>LUCAS PEREIRA DE MEDEIROS</td>
                <td>10,0</td>
            </tr>
            <tr>
                <td>11201921617</td>
                <td>LUCAS SANCHEZ BITENCOURT</td>
                <td>10,0</td>
            </tr>
            <tr>
                <td>11201920483</td>
                <td>MARCELA CESCHIM CABURLAO</td>
                <td>8,3</td>
            </tr>
            <tr>
                <td>11201921777</td>
                <td>MARCOS BALDRIGUE ANDRADE</td>
                <td>10,0</td>
            </tr>
            <tr>
                <td>21055813</td>
                <td>MIKAEL ALVES MONTEIRO</td>
                <td>10,0</td>
            </tr>
            <tr>
                <td>CONVIDADO</td>
                <td>MARCO ANTONIO DE CAMPOS</td>
                <td>8,3</td>
            </tr>
            <tr>
                <td>CONVIDADO</td>
                <td>IRIS REGINA CORNETTA DE CAMPOS</td>
                <td>10,0</td>
            </tr>
        </table>
        </div>
        
        <p>
        De forma geral, os resultados mostram um alto nível de acerto, com a maioria dos participantes alcançando a nota máxima (10,0). 
        Alguns participantes obtiveram 8,3, indicando pequenas dificuldades em pontos específicos, mas ainda assim com bom desempenho. 
        Isso demonstra que o sistema e o questionário foram bem compreendidos pela maioria dos avaliados.
        </p>

        
        <h2>Conclusões</h2>

        <p>
        O projeto de Sistema de Controle de Acesso com Visão Computacional demonstrou ser uma solução viável e eficiente para aumentar a segurança em ambientes escolares e 
        corporativos. Com o uso de técnicas de calibração, filtragem e reconhecimento facial, foi possível implementar um sistema capaz de identificar usuários autorizados e 
        registrar acessos de forma automática, reduzindo falhas comuns em métodos tradicionais como senhas e cartões.
        </p>

        <p>
        Durante o desenvolvimento, foram adquiridos aprendizados valiosos no campo da visão computacional, como a importância do pré-processamento de imagens para aumentar a 
        precisão dos algoritmos, o papel fundamental da iluminação e do posicionamento da câmera na qualidade da detecção, além do entendimento prático de como redes neurais e 
        técnicas de machine learning podem ser aplicadas para reconhecimento facial em tempo real.
        </p>

        <p>
        Os testes em campo mostraram resultados positivos, com alta taxa de acertos e boa aceitação pelos participantes. Assim, concluímos que o sistema cumpre seu objetivo 
        de aliar tecnologia e segurança, apresentando grande potencial para aplicações reais e futuras melhorias com o avanço das técnicas de inteligência artificial.
        </p>


        <h2>Referências Bibliográficas</h2>
        <ul>
            <li>Documentação OpenCV (Python).</li>
            <li>Geometria da formação de imagens: <a href="https://learnopencv.com/geometry-of-image-formation/" target="_blank">https://learnopencv.com/geometry-of-image-formation/</a></li>
            <li>Calibração de Câmera com OpenCV: <a href="https://learnopencv.com/camera-calibration-using-opencv/" target="_blank">https://learnopencv.com/camera-calibration-using-opencv/</a></li>
            <li>Camera Calibration no OpenCV: <a href="https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html" target="_blank">https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html</a></li>
            <li>Toolbox Matlab: <a href="http://robots.stanford.edu/cs223b04/JeanYvesCalib/" target="_blank">http://robots.stanford.edu/cs223b04/JeanYvesCalib/</a></li>
            <li>Pinhole Camera Model: <a href="https://en.wikipedia.org/wiki/Pinhole_camera_model" target="_blank">https://en.wikipedia.org/wiki/Pinhole_camera_model</a></li>
            <li>Making A Low-Cost Stereo Camera Using OpenCV: <a href="https://learnopencv.com/making-a-low-cost-stereo-camera-using-opencv/" target="_blank">https://learnopencv.com/making-a-low-cost-stereo-camera-using-opencv/</a> | Código: <a href="https://github.com/spmallick/learnopencv/tree/master/stereo-camera" target="_blank">https://github.com/spmallick/learnopencv/tree/master/stereo-camera</a></li>
            <li>Introduction to Epipolar Geometry and Stereo Vision: <a href="https://learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/" target="_blank">https://learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/</a> | Código: <a href="https://github.com/spmallick/learnopencv/tree/master/EpipolarGeometryAndStereoVision" target="_blank">https://github.com/spmallick/learnopencv/tree/master/EpipolarGeometryAndStereoVision</a></li>
            <li>Understanding Lens Distortion: <a href="https://learnopencv.com/understanding-lens-distortion/" target="_blank">https://learnopencv.com/understanding-lens-distortion/</a> | Código: <a href="https://github.com/spmallick/learnopencv/tree/master/UnderstandingLensDistortion" target="_blank">https://github.com/spmallick/learnopencv/tree/master/UnderstandingLensDistortion</a></li>
            <li>C. Loop and Z. Zhang. Computing Rectifying Homographies for Stereo Vision. IEEE Conf. Computer Vision and Pattern Recognition, 1999.</li>
            <li>OpenCV. Feature Detection and Description - Sumário: <a href="https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html" target="_blank">https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html</a></li>
            <li>OpenCV. Entendendo sobre Features: <a href="https://docs.opencv.org/4.x/df/d54/tutorial_py_features_meaning.html" target="_blank">https://docs.opencv.org/4.x/df/d54/tutorial_py_features_meaning.html</a></li>
            <li>OpenCV. Detector de Harris: <a href="https://docs.opencv.org/4.x/dc/d0d/tutorial_py_features_harris.html" target="_blank">https://docs.opencv.org/4.x/dc/d0d/tutorial_py_features_harris.html</a></li>
            <li>OpenCV. Detector de Shi-Tomasi: <a href="https://docs.opencv.org/4.x/d4/d8c/tutorial_py_shi_tomasi.html" target="_blank">https://docs.opencv.org/4.x/d4/d8c/tutorial_py_shi_tomasi.html</a></li>
            <li>OpenCV. Introdução ao SIFT (Scale-Invariant Feature Transform): <a href="https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html" target="_blank">https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html</a></li>
            <li>OpenCV. Feature Matching + Homography to find Objects: <a href="https://docs.opencv.org/4.x/d1/de0/tutorial_py_feature_homography.html" target="_blank">https://docs.opencv.org/4.x/d1/de0/tutorial_py_feature_homography.html</a></li>
            <li>LearnOpenCV. Hough Transform with OpenCV (C++/Python): <a href="https://learnopencv.com/hough-transform-with-opencv-c-python/" target="_blank">https://learnopencv.com/hough-transform-with-opencv-c-python/</a></li>
            <li>LearnOpenCV. Block Matching For Dense Stereo Correspondence: <a href="https://learnopencv.com/depth-perception-using-stereo-camera-python-c/" target="_blank">https://learnopencv.com/depth-perception-using-stereo-camera-python-c/</a></li>
        </ul>


        <h2>Anexos </h2>
        <ul>
            <li><a id="projetopy" href="projeto.py" download>Código do Projeto em Python</a></li>
            <li><a id="paramsxml" href="params_py.xml" download>Parâmetros de calibração da câmera</a></li>
            <li><a id="datasetfaces" href="dataset_faces.zip" download>Dataset de rostos cadastrados</a></li>
        </ul>

    </section>

  

  
   
</body>
</html>
